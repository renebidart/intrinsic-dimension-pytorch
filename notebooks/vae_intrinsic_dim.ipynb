{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrinsic Dimension with VAE using Sparse projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.random_projection import SparseRandomProjection as SRP\n",
    "from scipy.sparse import find\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from utils.train_val import train_net #, train_epoch, validate_epoch, save_checkpoint\n",
    "from utils.train_val_vae import train_vae #, train_epoch, validate_epoch, save_checkpoint\n",
    "\n",
    "from utils.data_loaders import mnist_loaders\n",
    "\n",
    "# sys.path.append(str(Path.cwd().parent.parent))\n",
    "# from pytorch_testing.tests.unit_tests import Tester, SimpleNet\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "PATH = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseVAE(nn.Module):\n",
    "    ''' Optimize subspace of parameters of network defined as  using dense projection.\n",
    "    \n",
    "    d: intrinsic dimension size to test\n",
    "    layers: torch.nn.ModuleDict()\n",
    "    config: dict with same keys as layers, containing 3 objects: \n",
    "            'type' (str): type of layer\n",
    "            'params' (dict) : all the params to specify the layer (type, ...)\n",
    "            'activation' (dict) : if there is an activation after the layer\n",
    "    \n",
    "    ??? This still won't work for resnets, or any other more complex network\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d, input_dim=1, output_dim=1, latent_size=8, img_size=28, DEVICE='cuda'):\n",
    "        super(SparseVAE, self).__init__()\n",
    "        self.d = d\n",
    "        self.opt_basis = nn.Parameter(torch.zeros(self.d).to(DEVICE), requires_grad=True)\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        self.img_size = img_size\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.linear_size = int(16*(img_size/4)**2)\n",
    "        \n",
    "        self.layers = torch.nn.ModuleDict()\n",
    "\n",
    "        self.layers['enc_conv1'] = nn.Conv2d(self.input_dim, 32, kernel_size=5, stride=1, padding=2, bias=False)\n",
    "        self.layers['enc_bn1'] = nn.BatchNorm2d(32, track_running_stats=True)\n",
    "        self.layers['enc_act1'] = nn.ELU()\n",
    "        self.layers['enc_conv2'] = nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.layers['enc_bn2'] = nn.BatchNorm2d(32, track_running_stats=True)\n",
    "        self.layers['enc_act2'] = nn.ELU()\n",
    "        self.layers['enc_conv3'] = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.layers['enc_bn3'] = nn.BatchNorm2d(64, track_running_stats=True)\n",
    "        self.layers['enc_act3'] = nn.ELU()\n",
    "        self.layers['enc_conv4'] = nn.Conv2d(64, 16, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "\n",
    "        self.layers['dec_conv1'] = nn.ConvTranspose2d(16, 32, kernel_size=4, stride=1, padding=2,  output_padding=0, bias=False)\n",
    "        self.layers['dec_bn1'] = nn.BatchNorm2d(32, track_running_stats=True)\n",
    "        self.layers['dec_act1'] = nn.ELU()\n",
    "        self.layers['dec_conv2'] = nn.ConvTranspose2d(32, 16, kernel_size=5, stride=2, padding=1,  output_padding=1, bias=False)\n",
    "        self.layers['dec_bn2'] = nn.BatchNorm2d(16, track_running_stats=True)\n",
    "        self.layers['dec_act2'] = nn.ELU()\n",
    "        self.layers['dec_conv3'] = nn.ConvTranspose2d(16, 16, kernel_size=5, stride=2, padding=2,  output_padding=1, bias=False)\n",
    "        self.layers['dec_bn3'] = nn.BatchNorm2d(16, track_running_stats=True)\n",
    "        self.layers['dec_act3'] = nn.ELU()\n",
    "        self.layers['dec_conv4'] = nn.ConvTranspose2d(16, self.output_dim, kernel_size=3, stride=1, padding=1,  output_padding=0, bias=True)\n",
    "        \n",
    "        self.config = {}\n",
    "        self.config['fc_mu_logvar'] = 'linear'\n",
    "        self.config['fc_dec'] = 'linear'\n",
    "        self.config['enc_conv1'] = 'conv2d'\n",
    "        self.config['enc_bn1'] = 'batch_norm'\n",
    "        self.config['enc_act1'] = 'elu'\n",
    "        self.config['enc_conv2'] = 'conv2d'\n",
    "        self.config['enc_bn2'] = 'batch_norm'\n",
    "        self.config['enc_act2'] = 'elu'\n",
    "        self.config['enc_conv3'] = 'conv2d'\n",
    "        self.config['enc_bn3'] = 'batch_norm'\n",
    "        self.config['enc_act3'] = 'elu'\n",
    "        self.config['enc_conv4'] = 'conv2d'\n",
    "        self.config['dec_conv1'] = 'conv_transpose2d'\n",
    "        self.config['dec_bn1'] = 'batch_norm'\n",
    "        self.config['dec_act1'] = 'elu'\n",
    "        self.config['dec_conv2'] = 'conv_transpose2d'\n",
    "        self.config['dec_bn2'] = 'batch_norm'\n",
    "        self.config['dec_act2'] = 'elu'\n",
    "        self.config['dec_conv3'] = 'conv_transpose2d'\n",
    "        self.config['dec_bn3'] = 'batch_norm'\n",
    "        self.config['dec_act3'] = 'elu'\n",
    "        self.config['dec_conv4'] = 'conv_transpose2d'\n",
    "        self.layers['fc_mu_logvar'] = nn.Linear(self.linear_size, 2*self.latent_size)\n",
    "        self.layers['fc_dec'] = nn.Linear(self.latent_size, self.linear_size)\n",
    "        \n",
    "        # get full dimension\n",
    "        self.D = 0\n",
    "        for name, layer in self.layers.items():\n",
    "            if hasattr(layer, 'weight'):\n",
    "                self.D += torch.prod(torch.tensor(layer.weight.size()))\n",
    "                if layer.bias is not None:\n",
    "                    self.D += torch.prod(torch.tensor(layer.bias.size()))\n",
    "                layer.requires_grad = False # none of the layers will be updated\n",
    "            \n",
    "        self.D = self.D.item()\n",
    "        self.get_projection_matrix()\n",
    "        \n",
    "    def slice_sparse_tensor(self, sparse_t, idx, dim):\n",
    "        # some from https://stackoverflow.com/questions/50666440/column-row-slicing-a-torch-sparse-tensor\n",
    "\n",
    "        def compact1D(x):\n",
    "            \"\"\" NORMALIZE indices to begin at 0 for new sliced tensor\n",
    "            \"Compact\" values 1D uint tensor, so that all values are in [0, max(unique(x))].\n",
    "            \"\"\"\n",
    "            x_sorted, x_sorted_ind = torch.sort(x, descending=True)\n",
    "            x_sorted_unique, x_sorted_unique_ind = torch.unique(x_sorted, return_inverse=True)\n",
    "            x[x_sorted_ind] = x_sorted_unique_ind\n",
    "            return x\n",
    "\n",
    "        idx = torch.tensor(idx)\n",
    "        sparse_i = sparse_t._indices()\n",
    "        sparse_v = sparse_t._values()\n",
    "\n",
    "        # find indices that are in the sparse tensor\n",
    "        # getting an error, so do it on cpu. CPU is too much memory, so do more efficient np method\n",
    "        existing_idx = np.isin(sparse_i[dim].cpu().numpy(), idx.cpu().numpy())\n",
    "        existing_idx = torch.from_numpy(existing_idx).byte().to(DEVICE)\n",
    "        \n",
    "        existing_idx = existing_idx.nonzero().squeeze()\n",
    "        sparse_i = sparse_i.to(DEVICE)\n",
    "        \n",
    "        # keep only existing indices\n",
    "        v_sliced = sparse_v[existing_idx]\n",
    "\n",
    "        # grab the indices that we want to keep\n",
    "        i_sliced = sparse_i.index_select(dim=1, index=existing_idx)\n",
    "\n",
    "        # Building sparse result tensor:\n",
    "        i_sliced[0] = compact1D(i_sliced[0])\n",
    "        i_sliced[1] = compact1D(i_sliced[1])\n",
    "\n",
    "        # find the new size\n",
    "        size_sliced = torch.tensor(sparse_t.size())\n",
    "        size_sliced[dim] = len(idx)\n",
    "        size_sliced = (size_sliced[0], size_sliced[1]) \n",
    "        \n",
    "        subset_sparse_t = torch.sparse.FloatTensor(i_sliced.to(DEVICE), v_sliced.to(DEVICE), size_sliced)\n",
    "        return subset_sparse_t.to(DEVICE)\n",
    "    \n",
    "    def get_projection_matrix(self):\n",
    "        M = SRP(self.d)._make_random_matrix(self.D, self.d)\n",
    "        M = normalize(M, norm='l2', axis=0)\n",
    "        fm=find(M)\n",
    "        \n",
    "        idx = torch.LongTensor(np.array([fm[0],fm[1]]))\n",
    "        vals = torch.tensor(fm[2])\n",
    "        proj_matrix = torch.sparse.FloatTensor(idx, vals, (self.D, self.d))\n",
    "        \n",
    "        # Turn it into a nice format for a weight and bias matrix for each layer\n",
    "        p_idx = 0\n",
    "        self.projection = {}\n",
    "        for name, layer in self.layers.items():\n",
    "            if hasattr(layer, 'weight'): # ignore anything that doesn't have params like act\n",
    "                self.projection[name] = {}\n",
    "                n_weights = torch.prod(torch.tensor(layer.weight.size()))\n",
    "                self.projection[name]['weight'] = self.slice_sparse_tensor(proj_matrix, \n",
    "                                                                      idx=list(range(p_idx, p_idx + n_weights)), \n",
    "                                                                      dim=0).float()\n",
    "                self.projection[name]['weight'].requires_grad = False\n",
    "                # also make sure the layers aren't trainable\n",
    "                self.layers[name].weight.requires_grad = False\n",
    "                p_idx += n_weights\n",
    "                if layer.bias is not None:\n",
    "                    n_weights = torch.prod(torch.tensor(layer.bias.size()))\n",
    "                    self.projection[name]['bias'] = self.slice_sparse_tensor(proj_matrix, \n",
    "                                                                      idx=list(range(p_idx, p_idx + n_weights)), \n",
    "                                                                      dim=0).float()\n",
    "                    self.projection[name]['bias'].requires_grad = False\n",
    "                    # also make sure the layers aren't trainable\n",
    "                    self.layers[name].bias.requires_grad = False\n",
    "                    p_idx += n_weights\n",
    "\n",
    "    def get_layer_activation(self, name):\n",
    "        mapping={'linear':F.linear,\n",
    "                 'conv2d': F.conv2d,\n",
    "                 'conv_transpose2d': F.conv_transpose2d,\n",
    "                 'relu': F.relu,\n",
    "                 'elu': F.elu,\n",
    "                 'batch_norm': F.batch_norm\n",
    "                }\n",
    "        return mapping[name] if name is not None else None\n",
    "\n",
    "    def encode(self, x):\n",
    "        for name, nn_layer in self.layers.items():\n",
    "            if name[0:3] == 'enc':\n",
    "                params = vars(self.layers[name]) #self.config[name]['params'] vars(x)\n",
    "                params = {k: v for k, v in params.items() if k in ['stride', 'padding', 'dilation', 'groups',\n",
    "                                                                    'running_mean', 'running_var']}\n",
    "                layer = self.get_layer_activation(self.config[name])\n",
    "\n",
    "                if self.config[name] in ['linear', 'conv2d', 'conv_transpose2d', 'batch_norm']:\n",
    "                    new_weight = self.layers[name].weight + torch.sparse.mm(self.projection[name]['weight'], \n",
    "                                                                            self.opt_basis.unsqueeze(-1)\n",
    "                                                               ).view(self.layers[name].weight.size())\n",
    "                    params['weight'] = new_weight\n",
    "\n",
    "                    if self.layers[name].bias is not None:\n",
    "                        new_bias = self.layers[name].bias + torch.sparse.mm(self.projection[name]['bias'], \n",
    "                                                                            self.opt_basis.unsqueeze(-1)\n",
    "                                                                   ).view(self.layers[name].bias.size())\n",
    "                        params['bias'] = new_bias\n",
    "\n",
    "                else: # this is an activation, so no params\n",
    "                    pass\n",
    "                \n",
    "                if self.config[name] is 'batch_norm':\n",
    "                    nn_layer.weight = nn.Parameter(params['weight'])\n",
    "                    nn_layer.bias = nn.Parameter(params['bias'])\n",
    "                    x = nn_layer(input=x)\n",
    "                else:\n",
    "                    x = layer(input=x, **params)\n",
    "               \n",
    "        # final FC layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        params = vars(self.layers['fc_mu_logvar']) #self.config[name]['params'] vars(x)\n",
    "        params = {k: v for k, v in params.items() if k in ['stride', 'padding', 'dilation', 'groups',\n",
    "                                                                'running_mean', 'running_var']}\n",
    "        layer = self.get_layer_activation(self.config['fc_mu_logvar'])\n",
    "        new_weight = self.layers['fc_mu_logvar'].weight + torch.sparse.mm(self.projection['fc_mu_logvar']['weight'], \n",
    "                                                                self.opt_basis.unsqueeze(-1)\n",
    "                                                   ).view(self.layers['fc_mu_logvar'].weight.size())\n",
    "        new_bias = self.layers['fc_mu_logvar'].bias + torch.sparse.mm(self.projection['fc_mu_logvar']['bias'], \n",
    "                                                            self.opt_basis.unsqueeze(-1)\n",
    "                                                   ).view(self.layers['fc_mu_logvar'].bias.size())\n",
    "        \n",
    "        params['weight'] = new_weight\n",
    "        params['bias'] = new_bias\n",
    "        mu_logvar = layer(input=x, **params) \n",
    "        return mu_logvar\n",
    "\n",
    "\n",
    "    def decode(self, x):\n",
    "        params = vars(self.layers['fc_dec']) #self.config[name]['params'] vars(x)\n",
    "        params = {k: v for k, v in params.items() if k in ['stride', 'padding', 'dilation', 'groups',\n",
    "                                                              'running_mean', 'running_var', 'output_padding']}\n",
    "        layer = self.get_layer_activation(self.config['fc_dec'])\n",
    "        new_weight = self.layers['fc_dec'].weight + torch.sparse.mm(self.projection['fc_dec']['weight'], \n",
    "                                                                self.opt_basis.unsqueeze(-1)\n",
    "                                                   ).view(self.layers['fc_dec'].weight.size())\n",
    "        new_bias = self.layers['fc_dec'].bias + torch.sparse.mm(self.projection['fc_dec']['bias'], \n",
    "                                                            self.opt_basis.unsqueeze(-1)\n",
    "                                                   ).view(self.layers['fc_dec'].bias.size())\n",
    "        params['weight'] = new_weight\n",
    "        params['bias'] = new_bias\n",
    "        x = layer(input=x, **params)\n",
    "        x = x.view((-1, 16, int(self.img_size/4), int(self.img_size/4)))\n",
    "\n",
    "        for name, nn_layer in self.layers.items():\n",
    "            if name[0:3] == 'dec':\n",
    "                params = vars(self.layers[name]) #self.config[name]['params'] vars(x)\n",
    "                params = {k: v for k, v in params.items() if k in ['stride', 'padding', 'dilation', 'groups',\n",
    "                                                                     'running_mean', 'running_var', 'output_padding']}\n",
    "                layer = self.get_layer_activation(self.config[name])\n",
    "\n",
    "                if self.config[name] in ['linear', 'conv2d', 'conv_transpose2d', 'batch_norm']:\n",
    "                    new_weight = self.layers[name].weight + torch.sparse.mm(self.projection[name]['weight'], \n",
    "                                                                            self.opt_basis.unsqueeze(-1)\n",
    "                                                               ).view(self.layers[name].weight.size())\n",
    "                    params['weight'] = new_weight\n",
    "                    if self.layers[name].bias is not None:\n",
    "                        new_bias = self.layers[name].bias + torch.sparse.mm(self.projection[name]['bias'], \n",
    "                                                                            self.opt_basis.unsqueeze(-1)\n",
    "                                                                   ).view(self.layers[name].bias.size())\n",
    "                        params['bias'] = new_bias\n",
    "                else: # this is an activation, so no params\n",
    "                    pass\n",
    "                if self.config[name] is 'batch_norm':\n",
    "                    nn_layer.weight = nn.Parameter(params['weight'])\n",
    "                    nn_layer.bias = nn.Parameter(params['bias'])\n",
    "                    x = nn_layer(input=x)\n",
    "                else:\n",
    "                    x = layer(input=x, **params)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "    def reparameterize(self, mu_logvar, deterministic=False):\n",
    "        mu = mu_logvar[:, 0:int(mu_logvar.size()[1]/2)]\n",
    "        if deterministic: # return mu \n",
    "            return mu\n",
    "        else: # return mu + random\n",
    "            logvar = mu_logvar[:, int(mu_logvar.size()[1]/2):]\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        \n",
    "    def forward(self, x, deterministic=False):\n",
    "        mu_logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu_logvar, deterministic)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 1/3 [03:20<06:41, 200.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 2/3 [06:42<03:20, 201.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [10:02<00:00, 200.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 619.6574358398437\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = mnist_loaders(PATH, bs=256)\n",
    "model = SparseVAE(d=8192).to(DEVICE)\n",
    "metrics = train_vae(model, train_loader, val_loader, epochs=3, verbose=1, device=DEVICE)\n",
    "print('Best Loss:', metrics['val']['best_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x = x.to(DEVICE)\n",
    "x_recon, mu_logvar = model(x)\n",
    "\n",
    "plt.imshow(x[1, :, :, :].squeeze().cpu().detach().numpy())\n",
    "plt.show()\n",
    "plt.imshow(x_recon[1, :, :, :].squeeze().cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 1/6 [03:22<16:51, 202.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 2/6 [06:44<13:29, 202.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 3/6 [10:07<10:07, 202.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 4/6 [13:30<06:44, 202.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 5/6 [16:52<03:22, 202.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 6/6 [20:15<00:00, 202.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 566.2166041992188\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = mnist_loaders(PATH, bs=256)\n",
    "model = SparseVAE(d=8192).to(DEVICE)\n",
    "metrics = train_vae(model, train_loader, val_loader, epochs=6, verbose=1, device=DEVICE)\n",
    "print('Best Loss:', metrics['val']['best_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x = x.to(DEVICE)\n",
    "x_recon, mu_logvar = model(x)\n",
    "\n",
    "plt.imshow(x[1, :, :, :].squeeze().cpu().detach().numpy())\n",
    "plt.show()\n",
    "plt.imshow(x_recon[1, :, :, :].squeeze().cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ATraceback (most recent call last):\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/rene/miniconda3/envs/HVAE/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-2c98d6e9df32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparseVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best Loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/rene/code/intrinsic-dimension-pytorch/utils/train_val_vae.py\u001b[0m in \u001b[0;36mtrain_vae\u001b[0;34m(model, train_loader, val_loader, epochs, lr, verbose, SAVE_PATH, model_name, save_freq, log_interval, device)\u001b[0m\n\u001b[1;32m     98\u001b[0m                                        \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                                        \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvae_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                                        device=device)        \n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         val_metrics  = val_epoch_vae(epoch=epoch, \n",
      "\u001b[0;32m/media/rene/code/intrinsic-dimension-pytorch/utils/train_val_vae.py\u001b[0m in \u001b[0;36mtrain_epoch_vae\u001b[0;34m(epoch, train_loader, model, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/HVAE/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/HVAE/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = mnist_loaders(PATH, bs=256)\n",
    "model = SparseVAE(d=8192).to(DEVICE)\n",
    "metrics = train_vae(model, train_loader, val_loader, epochs=13, verbose=1, device=DEVICE)\n",
    "print('Best Loss:', metrics['val']['best_loss'])\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "x = x.to(DEVICE)\n",
    "x_recon, mu_logvar = model(x)\n",
    "\n",
    "plt.imshow(x[1, :, :, :].squeeze().cpu().detach().numpy())\n",
    "plt.show()\n",
    "plt.imshow(x_recon[1, :, :, :].squeeze().cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HVAE",
   "language": "python",
   "name": "hvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
