{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrinsic Dimension\n",
    "* Dense and sparse projections\n",
    "?https://discuss.pytorch.org/t/fast-walsh-hadamard-transform/19341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(str(Path.cwd().parent))\n",
    "# from utils.train_val import train_net #, train_epoch, validate_epoch, save_checkpoint\n",
    "# from utils.data_loaders import mnist_loaders\n",
    "\n",
    "# # sys.path.append(str(Path.cwd().parent.parent))\n",
    "# # from pytorch_testing.tests.unit_tests import Tester, SimpleNet\n",
    "\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "# %matplotlib inline\n",
    "\n",
    "# DEVICE = torch.device(\"cuda:1\")\n",
    "# PATH = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.random_projection import SparseRandomProjection as SRP\n",
    "from scipy.sparse import find\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from utils.train_val import train_net #, train_epoch, validate_epoch, save_checkpoint\n",
    "from utils.data_loaders import mnist_loaders\n",
    "\n",
    "# sys.path.append(str(Path.cwd().parent.parent))\n",
    "# from pytorch_testing.tests.unit_tests import Tester, SimpleNet\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "PATH = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallFCNetMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallFCNetMNIST, self).__init__()\n",
    "        self.layers = torch.nn.ModuleDict()\n",
    "        self.layers['fc1'] = nn.Linear(28*28, 128)\n",
    "        self.layers['fc2'] = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.layers['fc1'](x))\n",
    "        x = self.layers['fc2'](x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = mnist_loaders(PATH, bs=256)\n",
    "model = SmallFCNetMNIST().to(DEVICE)\n",
    "metrics = train_net(model, train_loader, val_loader, epochs=30, verbose=0, device=DEVICE)\n",
    "print('SmallFCNetMNIST Acc with all params:', metrics['val']['best_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense matrix projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseProjSmallFCNetMNIST(nn.Module):        \n",
    "    def __init__(self, d, DEVICE='cuda'):\n",
    "        super(DenseProjSmallFCNetMNIST, self).__init__()\n",
    "        self.layers = torch.nn.ModuleDict()\n",
    "        self.layers['fc1'] = nn.Linear(28*28, 128)\n",
    "        self.layers['fc2'] = nn.Linear(128, 10)\n",
    "        self.d = d\n",
    "#         self.opt_basis= torch.zeros(self.d).to(DEVICE)\n",
    "        self.opt_basis = nn.Parameter(torch.zeros(self.d).to(DEVICE), requires_grad=True)\n",
    "\n",
    "        self.get_projection_matrix()\n",
    "            \n",
    "    def get_projection_matrix(self):\n",
    "        self.D = 0\n",
    "        for name, layer in self.layers.items():\n",
    "            self.D += torch.prod(torch.tensor(layer.weight.size()))\n",
    "            if layer.bias is not None:\n",
    "                self.D += torch.prod(torch.tensor(layer.bias.size()))\n",
    "            layer.requires_grad = False # none of the layers will be updated\n",
    "        \n",
    "        proj_matrix = nn.Parameter(torch.randn(self.D, self.d).to(DEVICE), requires_grad=False)\n",
    "        proj_matrix = F.normalize(proj_matrix, dim=0, p=2)\n",
    "        \n",
    "        # Turn it into a nice format for a weight and bias matrix for each layer\n",
    "        param_idx = 0\n",
    "        self.projection = {}\n",
    "        for name, layer in self.layers.items():\n",
    "            self.projection[name] = {}\n",
    "            n_weight_params = torch.prod(torch.tensor(layer.weight.size()))\n",
    "            self.projection[name]['weight'] = proj_matrix[param_idx : param_idx + n_weight_params, :]\n",
    "            self.projection[name]['weight'].requires_grad = False\n",
    "            # also make sure the layers aren't trainable\n",
    "            self.layers[name].weight.requires_grad = False\n",
    "            param_idx += n_weight_params\n",
    "            if layer.bias is not None:\n",
    "                n_bias_params = torch.prod(torch.tensor(layer.bias.size()))\n",
    "                self.projection[name]['bias'] = proj_matrix[param_idx : param_idx + n_bias_params, :]\n",
    "                self.projection[name]['bias'].requires_grad = False\n",
    "                # also make sure the layers aren't trainable\n",
    "                self.layers[name].bias.requires_grad = False\n",
    "                param_idx += n_bias_params\n",
    "#         print(f'Model contains {self.D} params, but only optimizing a {self.d} dim subspace')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        new_weight = self.layers['fc1'].weight + torch.matmul(self.projection['fc1']['weight'], self.opt_basis\n",
    "                                                   ).view(self.layers['fc1'].weight.size())\n",
    "        new_bias = self.layers['fc1'].bias + torch.matmul(self.projection['fc1']['bias'], self.opt_basis\n",
    "                                                   ).view(self.layers['fc1'].bias.size())\n",
    "        x = F.linear(input = x, \n",
    "                     weight=new_weight, \n",
    "                     bias=new_bias\n",
    "                    )\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        new_weight = self.layers['fc2'].weight + torch.matmul(self.projection['fc2']['weight'], self.opt_basis\n",
    "                                                   ).view(self.layers['fc2'].weight.size())\n",
    "        new_bias = self.layers['fc2'].bias + torch.matmul(self.projection['fc2']['bias'], self.opt_basis\n",
    "                                                   ).view(self.layers['fc2'].bias.size())\n",
    "        x = F.linear(input = x,\n",
    "                     weight=new_weight, \n",
    "                     bias=new_bias\n",
    "                    )\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseProjSmallFCNetMNIST Acc with 1024 params: 0.9064\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = mnist_loaders(PATH, bs=256)\n",
    "model = DenseProjSmallFCNetMNIST(d=1024, DEVICE=DEVICE).to(DEVICE)\n",
    "metrics = train_net(model, train_loader, val_loader, epochs=30, verbose=0, device=DEVICE)\n",
    "print(f'DenseProjSmallFCNetMNIST Acc with 1024 params: {metrics[\"val\"][\"best_acc\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseProjSmallFCNetMNIST Acc with 8 params: 0.1602\n",
      "DenseProjSmallFCNetMNIST Acc with 16 params: 0.2389\n",
      "DenseProjSmallFCNetMNIST Acc with 32 params: 0.3351\n",
      "DenseProjSmallFCNetMNIST Acc with 64 params: 0.5496\n",
      "DenseProjSmallFCNetMNIST Acc with 128 params: 0.6981\n",
      "DenseProjSmallFCNetMNIST Acc with 256 params: 0.8053\n",
      "DenseProjSmallFCNetMNIST Acc with 512 params: 0.8829\n",
      "DenseProjSmallFCNetMNIST Acc with 1024 params: 0.9248\n",
      "DenseProjSmallFCNetMNIST Acc with 2056 params: 0.9486\n"
     ]
    }
   ],
   "source": [
    "d_list = [8, 16, 32, 64, 128, 256, 512, 1024, 2056]\n",
    "results = []\n",
    "for d in d_list:\n",
    "    train_loader, val_loader = mnist_loaders(PATH, bs=256)\n",
    "    model = DenseProjSmallFCNetMNIST(d=d, DEVICE=DEVICE).to(DEVICE)\n",
    "    metrics = train_net(model, train_loader, val_loader, epochs=30, verbose=0, device=DEVICE)\n",
    "    print(f'DenseProjSmallFCNetMNIST Acc with {d} params: {metrics[\"val\"][\"best_acc\"]}')\n",
    "    results.append(metrics[\"val\"][\"best_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('seaborn-colorblind')\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(d_list, results)#, label='', linewidth=2)\n",
    "\n",
    "# ax.set_ylim([560,740])\n",
    "ax.set_xlabel('Optimization Dimension', fontsize='large')\n",
    "ax.set_ylabel('Accuracy', fontsize='large')\n",
    "# ax.legend(loc='best', prop={'size': 10})\n",
    "# ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "plt.savefig('../output/imgs/DenseProjSmallFCNetMNIST.png', bbox='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with sparse projection\n",
    "* Implement a slightly more general model to compute intrinsic dimension for a CNN. \n",
    "* Use a sparse matrix instead of dense for projection to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntrinsicNetSparse(nn.Module):\n",
    "    ''' Optimize subspace of parameters of network defined as  using dense projection.\n",
    "    \n",
    "    d: intrinsic dimension size to test\n",
    "    layers: torch.nn.ModuleDict()\n",
    "    config: dict with same keys as layers, containing 3 objects: \n",
    "            'type' (str): type of layer\n",
    "            'params' (dict) : all the params to specify the layer (type, ...)\n",
    "            'activation' (dict) : if there is an activation after the layer\n",
    "    \n",
    "    ??? This still won't work for resnets, or any other more complex network\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d, layers, config, DEVICE='cuda'):\n",
    "        super(IntrinsicNetSparse, self).__init__()\n",
    "        self.d = d\n",
    "        self.layers = layers\n",
    "        self.config = config\n",
    "        self.opt_basis = nn.Parameter(torch.zeros(self.d).to(DEVICE), requires_grad=True)\n",
    "        \n",
    "        self.D = 0\n",
    "        for name, layer in self.layers.items():\n",
    "            self.D += torch.prod(torch.tensor(layer.weight.size()))\n",
    "            if layer.bias is not None:\n",
    "                self.D += torch.prod(torch.tensor(layer.bias.size()))\n",
    "            layer.requires_grad = False # none of the layers will be updated\n",
    "        self.D = self.D.item()\n",
    "\n",
    "        self.get_projection_matrix()\n",
    "        \n",
    "    def slice_sparse_tensor(self, sparse_t, idx, dim):\n",
    "        # some from https://stackoverflow.com/questions/50666440/column-row-slicing-a-torch-sparse-tensor\n",
    "\n",
    "#         def isin(ar1, ar2): Too much memory, even on cpu\n",
    "#             print('ar1.device', ar1.device)\n",
    "#             print('ar2.device', ar2.device)\n",
    "#             return (ar1[..., None] == ar2).any(-1)\n",
    "\n",
    "        def compact1D(x):\n",
    "            \"\"\" NORMALIZE indices to begin at 0 for new sliced tensor\n",
    "            \"Compact\" values 1D uint tensor, so that all values are in [0, max(unique(x))].\n",
    "            \"\"\"\n",
    "            x_sorted, x_sorted_ind = torch.sort(x, descending=True)\n",
    "            x_sorted_unique, x_sorted_unique_ind = torch.unique(x_sorted, return_inverse=True)\n",
    "            x[x_sorted_ind] = x_sorted_unique_ind\n",
    "            return x\n",
    "\n",
    "        idx = torch.tensor(idx)\n",
    "        sparse_i = sparse_t._indices()\n",
    "        sparse_v = sparse_t._values()\n",
    "\n",
    "        # find indices that are in the sparse tensor\n",
    "        # getting an error, so do it on cpu. CPU is too much memory, so do more efficient np method\n",
    "        existing_idx = np.isin(sparse_i[dim].cpu().numpy(), idx.cpu().numpy())\n",
    "        existing_idx = torch.from_numpy(existing_idx).byte().to(DEVICE)\n",
    "        \n",
    "        existing_idx = existing_idx.nonzero().squeeze()\n",
    "        sparse_i = sparse_i.to(DEVICE)\n",
    "        \n",
    "        # keep only existing indices\n",
    "        v_sliced = sparse_v[existing_idx]\n",
    "\n",
    "        # grab the indices that we want to keep\n",
    "        i_sliced = sparse_i.index_select(dim=1, index=existing_idx)\n",
    "\n",
    "        # Building sparse result tensor:\n",
    "        i_sliced[0] = compact1D(i_sliced[0])\n",
    "        i_sliced[1] = compact1D(i_sliced[1])\n",
    "\n",
    "        # find the new size\n",
    "        size_sliced = torch.tensor(sparse_t.size())\n",
    "        size_sliced[dim] = len(idx)\n",
    "        size_sliced = (size_sliced[0], size_sliced[1]) \n",
    "        \n",
    "        subset_sparse_t = torch.sparse.FloatTensor(i_sliced.to(DEVICE), v_sliced.to(DEVICE), size_sliced)\n",
    "        return subset_sparse_t.to(DEVICE)\n",
    "    \n",
    "    def get_projection_matrix(self):\n",
    "        M = SRP(self.d)._make_random_matrix(self.D, self.d)\n",
    "        M = normalize(M, norm='l2', axis=0)\n",
    "        fm=find(M)\n",
    "        \n",
    "        idx = torch.LongTensor(np.array([fm[0],fm[1]]))\n",
    "        vals = torch.tensor(fm[2])\n",
    "        proj_matrix = torch.sparse.FloatTensor(idx, vals, (self.D, self.d))\n",
    "        \n",
    "        # Turn it into a nice format for a weight and bias matrix for each layer\n",
    "        p_idx = 0\n",
    "        self.projection = {}\n",
    "        for name, layer in self.layers.items():\n",
    "            self.projection[name] = {}\n",
    "            n_weights = torch.prod(torch.tensor(layer.weight.size()))\n",
    "            self.projection[name]['weight'] = self.slice_sparse_tensor(proj_matrix, \n",
    "                                                                  idx=list(range(p_idx, p_idx + n_weights)), \n",
    "                                                                  dim=0).float()\n",
    "            self.projection[name]['weight'].requires_grad = False\n",
    "            # also make sure the layers aren't trainable\n",
    "            self.layers[name].weight.requires_grad = False\n",
    "            p_idx += n_weights\n",
    "            if layer.bias is not None:\n",
    "                n_weights = torch.prod(torch.tensor(layer.bias.size()))\n",
    "                self.projection[name]['bias'] = self.slice_sparse_tensor(proj_matrix, \n",
    "                                                                  idx=list(range(p_idx, p_idx + n_weights)), \n",
    "                                                                  dim=0).float()\n",
    "                self.projection[name]['bias'].requires_grad = False\n",
    "                # also make sure the layers aren't trainable\n",
    "                self.layers[name].bias.requires_grad = False\n",
    "                p_idx += n_weights\n",
    "\n",
    "    def get_layer_activation(self, name):\n",
    "        mapping={'linear':F.linear,\n",
    "                 'conv2d': F.conv2d,\n",
    "                 'relu': F.relu\n",
    "                }\n",
    "        return mapping[name] if name is not None else None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for name, layer in self.layers.items():\n",
    "            if self.config[name]['layer_type'] in ['linear', 'conv2d']:\n",
    "                new_weight = self.layers[name].weight + torch.sparse.mm(self.projection[name]['weight'], \n",
    "                                                                        self.opt_basis.unsqueeze(-1)\n",
    "                                                           ).view(self.layers[name].weight.size())\n",
    "                new_bias = self.layers[name].bias + torch.sparse.mm(self.projection[name]['bias'], \n",
    "                                                                    self.opt_basis.unsqueeze(-1)\n",
    "                                                           ).view(self.layers[name].bias.size())\n",
    "\n",
    "                layer = self.get_layer_activation(self.config[name]['layer_type'])\n",
    "                \n",
    "                params = self.config[name]['params']\n",
    "                \n",
    "                params['weight'] = new_weight\n",
    "                params['bias'] = new_bias\n",
    "                    \n",
    "            elif self.config[name]['layer_type'] == 'bn':\n",
    "                pass\n",
    "                \n",
    "            activation = self.get_layer_activation(self.config[name]['activation'])\n",
    "            \n",
    "            x = layer(input=x, **params)\n",
    "            if activation is not None:\n",
    "                x = activation(x)\n",
    "        x = x.squeeze()\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [10:38<24:49, 212.75s/it]"
     ]
    }
   ],
   "source": [
    "layers = torch.nn.ModuleDict()\n",
    "layers['conv1'] = nn.Conv2d(1, 32, 5, stride=2, padding=2)\n",
    "layers['conv2'] = nn.Conv2d(32, 128, 5, stride=2, padding=2)\n",
    "layers['conv3'] = nn.Conv2d(128, 256, 7, stride=1, padding=0)\n",
    "layers['conv4'] = nn.Conv2d(256, 10, 1, stride=1, padding=0)\n",
    "config = {}\n",
    "config['conv1'] = {'layer_type': 'conv2d',\n",
    "                   'activation': 'relu', \n",
    "                   'params': {\n",
    "                       'stride': 2, 'padding':2\n",
    "                   } \n",
    "                  }\n",
    "config['conv2'] = {'layer_type': 'conv2d',\n",
    "                   'activation': 'relu', \n",
    "                   'params': {\n",
    "                       'stride': 2, 'padding':2\n",
    "                   }\n",
    "                   }\n",
    "                   \n",
    "config['conv3'] = {'layer_type': 'conv2d',\n",
    "                   'activation': 'relu',\n",
    "                   'params': {\n",
    "                       'stride': 1, 'padding':0\n",
    "                   }\n",
    "                   }\n",
    "config['conv4'] = {'layer_type': 'conv2d',\n",
    "                   'activation': None,\n",
    "                   'params': {\n",
    "                       'stride': 1, 'padding':0\n",
    "                   }\n",
    "                   }\n",
    "                   \n",
    "train_loader, val_loader = mnist_loaders(PATH, bs=256)\n",
    "model = IntrinsicNetSparse(d=32, layers=layers, config=config, DEVICE=DEVICE).to(DEVICE)\n",
    "metrics = train_net(model, train_loader, val_loader, epochs=10, verbose=1, device=DEVICE)\n",
    "print(f'CNN Acc with 32 params: {metrics[\"val\"][\"best_acc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare intrinsic dimension of a few CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7c231670b3a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvsmall_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvsmall_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvsmall_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvsmall_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv3'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvsmall_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv4'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "vsmall_layers = torch.nn.ModuleDict()\n",
    "vsmall_layers['conv1'] = nn.Conv2d(1, 64, 5, stride=2, padding=2)\n",
    "vsmall_layers['conv2'] = nn.Conv2d(64, 100, 5, stride=2, padding=2)\n",
    "vsmall_layers['conv3'] = nn.Conv2d(100, 100, 7, stride=1, padding=0)\n",
    "vsmall_layers['conv4'] = nn.Conv2d(100, 10, 1, stride=1, padding=0)\n",
    "vsmall_config = {}\n",
    "vsmall_config['conv1'] = {'layer_type': 'conv2d',\n",
    "                       'activation': 'relu', \n",
    "                       'params': {\n",
    "                           'stride': 2, 'padding':2\n",
    "                       } \n",
    "                      }\n",
    "vsmall_config['conv2'] = {'layer_type': 'conv2d',\n",
    "                           'activation': 'relu', \n",
    "                           'params': {\n",
    "                               'stride': 2, 'padding':2\n",
    "                           }\n",
    "                           }\n",
    "\n",
    "vsmall_config['conv3'] = {'layer_type': 'conv2d',\n",
    "                           'activation': 'relu',\n",
    "                           'params': {\n",
    "                               'stride': 1, 'padding':0\n",
    "                           }\n",
    "                           } \n",
    "vsmall_config['conv4'] = {'layer_type': 'conv2d',\n",
    "                           'activation': None,\n",
    "                           'params': {\n",
    "                               'stride': 1, 'padding':0\n",
    "                           }\n",
    "                           }\n",
    "\n",
    "small_layers = torch.nn.ModuleDict()\n",
    "small_layers['conv1'] = nn.Conv2d(1, 128, 5, stride=2, padding=2)\n",
    "small_layers['conv2'] = nn.Conv2d(128, 256, 5, stride=2, padding=2)\n",
    "small_layers['conv3'] = nn.Conv2d(256, 256, 7, stride=1, padding=0)\n",
    "small_layers['conv4'] = nn.Conv2d(256, 10, 1, stride=1, padding=0)\n",
    "small_config = {}\n",
    "small_config['conv1'] = {'layer_type': 'conv2d',\n",
    "                       'activation': 'relu', \n",
    "                       'params': {\n",
    "                           'stride': 2, 'padding':2\n",
    "                       } \n",
    "                      }\n",
    "small_config['conv2'] = {'layer_type': 'conv2d',\n",
    "                           'activation': 'relu', \n",
    "                           'params': {\n",
    "                               'stride': 2, 'padding':2\n",
    "                           }\n",
    "                           }\n",
    "\n",
    "small_config['conv3'] = {'layer_type': 'conv2d',\n",
    "                           'activation': 'relu',\n",
    "                           'params': {\n",
    "                               'stride': 1, 'padding':0\n",
    "                           }\n",
    "                           } \n",
    "small_config['conv4'] = {'layer_type': 'conv2d',\n",
    "                           'activation': None,\n",
    "                           'params': {\n",
    "                               'stride': 1, 'padding':0\n",
    "                           }\n",
    "                           }\n",
    "\n",
    "med_layers = torch.nn.ModuleDict()\n",
    "med_layers['conv1'] = nn.Conv2d(1, 128, 5, stride=2, padding=2)\n",
    "med_layers['conv2'] = nn.Conv2d(128, 128, 5, stride=1, padding=2)\n",
    "med_layers['conv3'] = nn.Conv2d(256, 512, 5, stride=2, padding=2)\n",
    "med_layers['conv4'] = nn.Conv2d(512, 512, 7, stride=1, padding=0)\n",
    "med_layers['conv5'] = nn.Conv2d(512, 10, 1, stride=1, padding=0)\n",
    "med_config = {}\n",
    "med_config['conv1'] = {'layer_type': 'conv2d',\n",
    "                       'activation': 'relu', \n",
    "                       'params': {\n",
    "                           'stride': 2, 'padding':2\n",
    "                       } \n",
    "                      }\n",
    "med_config['conv2'] = {'layer_type': 'conv2d',\n",
    "                           'activation': 'relu', \n",
    "                           'params': {\n",
    "                               'stride': 1, 'padding':2\n",
    "                           }\n",
    "                           }\n",
    "med_config['conv3'] = {'layer_type': 'conv2d',\n",
    "                           'activation': 'relu', \n",
    "                           'params': {\n",
    "                               'stride': 2, 'padding':2\n",
    "                           }\n",
    "                           }\n",
    "med_config['conv4'] = {'layer_type': 'conv2d',\n",
    "                           'activation': 'relu',\n",
    "                           'params': {\n",
    "                               'stride': 1, 'padding':0\n",
    "                           }\n",
    "                           } \n",
    "med_config['conv5'] = {'layer_type': 'conv2d',\n",
    "                           'activation': None,\n",
    "                           'params': {\n",
    "                               'stride': 1, 'padding':0\n",
    "                           }\n",
    "                           }\n",
    "\n",
    "\n",
    "results = {}\n",
    "results['d_list'] = [16, 32, 64, 128, 256, 512, 1024, 2056]\n",
    "results['vsmall'] = []\n",
    "results['small'] = []\n",
    "results['med'] = []\n",
    "\n",
    "for d in results['d_list']:\n",
    "    train_loader, val_loader = mnist_loaders(PATH, bs=256)\n",
    "    \n",
    "    model = IntrinsicNetSparse(d=d, layers=vsmall_layers, config=vsmall_config, DEVICE=DEVICE).to(DEVICE)\n",
    "    metrics = train_net(model, train_loader, val_loader, epochs=30, verbose=0, device=DEVICE)\n",
    "    results['vsmall'].append(metrics[\"val\"][\"best_acc\"])\n",
    "    \n",
    "    model = IntrinsicNetSparse(d=d, layers=small_layers, config=small_config, DEVICE=DEVICE).to(DEVICE)\n",
    "    metrics = train_net(model, train_loader, val_loader, epochs=30, verbose=0, device=DEVICE)\n",
    "    results['small'].append(metrics[\"val\"][\"best_acc\"])\n",
    "    \n",
    "    model = IntrinsicNetSparse(d=1024, layers=med_layers, config=med_config, DEVICE=DEVICE).to(DEVICE)\n",
    "    metrics = train_net(model, train_loader, val_loader, epochs=30, verbose=1, device=DEVICE)\n",
    "    results['med'].append(metrics[\"val\"][\"best_acc\"])\n",
    "    \n",
    "    pd.DataFrame.from_dict(results).to_csv('../output/CNN_compare_MNIST.csv')\n",
    "          \n",
    "plt.style.use('seaborn-colorblind')\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(d_list, results['vsmall'], label='', linewidth=2)\n",
    "ax.plot(d_list, results['small'], label='', linewidth=2)\n",
    "ax.plot(d_list, results['med'], label='', linewidth=2)\n",
    "ax.set_xlabel('Optimization Subspace Dimension', fontsize='large')\n",
    "ax.set_ylabel('Accuracy', fontsize='large')\n",
    "ax.legend(loc='best', prop={'size': 10})\n",
    "plt.savefig('../output/imgs/CNN_compare_MNIST.png', bbox='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('../output/CNN_compare_MNIST.csv')\n",
    "\n",
    "img = '../output/imgs/CNN_compare_MNIST.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_i tensor([[0, 1, 2],\n",
      "        [1, 0, 1]])\n",
      "sparse_i.size() torch.Size([2, 3])\n",
      "sparse_v tensor([3., 4., 5.])\n",
      "existing_idx tensor([0, 1, 1], dtype=torch.uint8)\n",
      "existing_idx tensor([1, 2])\n",
      "v_sliced tensor([4., 5.])\n",
      "i_sliced tensor([[1, 2],\n",
      "        [0, 1]])\n",
      "i_sliced tensor([[0, 1],\n",
      "        [0, 1]])\n",
      "size_sliced tensor([2, 2])\n",
      "size_sliced (tensor(2), tensor(2))\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "i = torch.LongTensor([[0, 1], [1, 0], [2, 1]])\n",
    "v = torch.FloatTensor([3,      4,      5    ])\n",
    "sparse_t = torch.sparse.FloatTensor(i.t(), v, torch.Size([3,2]))#.to_dense()\n",
    "idx = torch.tensor([1, 2])\n",
    "dim = 0\n",
    "\n",
    "\n",
    "def isin(ar1, ar2):\n",
    "    return (ar1[..., None] == ar2).any(-1)\n",
    "\n",
    "sparse_i = sparse_t._indices()\n",
    "sparse_v = sparse_t._values()\n",
    "\n",
    "print('sparse_i', sparse_i)\n",
    "print('sparse_i.size()', sparse_i.size())\n",
    "print('sparse_v', sparse_v)\n",
    "\n",
    "# print(sparse_i[0])\n",
    "# print(sparse_i[1])\n",
    "\n",
    "# find indices that are in the sparse tensor\n",
    "existing_idx = isin(sparse_i[dim], idx).byte()\n",
    "print('existing_idx', existing_idx)\n",
    "existing_idx = existing_idx.nonzero().squeeze()\n",
    "print('existing_idx', existing_idx)\n",
    "\n",
    "# keep only existing indices\n",
    "v_sliced = sparse_v[existing_idx]\n",
    "print('v_sliced', v_sliced)\n",
    "\n",
    "# grab the indices that we want to keep\n",
    "i_sliced = sparse_i.index_select(dim=1, index=existing_idx)\n",
    "\n",
    "# i_sliced = sparse_i.index_select(dim=dim, index=idx)\n",
    "print('i_sliced', i_sliced)\n",
    "\n",
    "def compact1D(x):\n",
    "    \"\"\" NORMALIZE indices to begin at 0 for new sliced tensor\n",
    "    \"Compact\" values 1D uint tensor, so that all values are in [0, max(unique(x))].\n",
    "    Args:\n",
    "        x (Tensor): uint Tensor\n",
    "\n",
    "    Returns:\n",
    "        Tensor: uint Tensor of same shape as x\n",
    "\n",
    "    Example:\n",
    "        >>> densify1D(torch.ByteTensor([5, 8, 7, 3, 8, 42]))\n",
    "        ByteTensor([1, 3, 2, 0, 3, 4])\n",
    "    \"\"\"\n",
    "    x_sorted, x_sorted_ind = torch.sort(x, descending=True)\n",
    "    x_sorted_unique, x_sorted_unique_ind = torch.unique(x_sorted, return_inverse=True)\n",
    "    x[x_sorted_ind] = x_sorted_unique_ind\n",
    "    return x\n",
    "\n",
    "# Building sparse result tensor:\n",
    "i_sliced[0] = compact1D(i_sliced[0])\n",
    "i_sliced[1] = compact1D(i_sliced[1])\n",
    "print('i_sliced', i_sliced)\n",
    "\n",
    "# To make sure to have a square dense representation:\n",
    "print('size_sliced', size_sliced)\n",
    "size_sliced = torch.tensor(sparse_t.size())\n",
    "size_sliced[dim] = len(idx)\n",
    "size_sliced = (size_sliced[0], size_sliced[1])\n",
    "print('size_sliced', size_sliced)\n",
    "# size_sliced[dim] = len(idx)\n",
    "\n",
    "print(type(i_sliced))\n",
    "print(type(v_sliced))\n",
    "print(type(size_sliced))\n",
    "\n",
    "\n",
    "subset_sparse_t = torch.sparse.FloatTensor(i_sliced, v_sliced, (size_sliced[0], size_sliced[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def in1D(x, labels):\n",
    "#     \"\"\" \n",
    "#     Sub-optimal equivalent to numpy.in1D().\n",
    "#     Hopefully this feature will be properly covered soon\n",
    "#     c.f. https://github.com/pytorch/pytorch/issues/3025\n",
    "#     Snippet by Aron Barreira Bordin\n",
    "#     Args:\n",
    "#         x (Tensor):             Tensor to search values in\n",
    "#         labels (Tensor/list):   1D array of values to search for\n",
    "\n",
    "#     Returns:\n",
    "#         Tensor: Boolean tensor y of same shape as x, with y[ind] = True if x[ind] in labels\n",
    "\n",
    "#     Example:\n",
    "#         >>> in1D(torch.FloatTensor([1, 2, 0, 3]), [2, 3])\n",
    "#         FloatTensor([False, True, False, True])\n",
    "#     \"\"\"\n",
    "#     mapping = torch.zeros(x.size()).byte()\n",
    "#     for label in labels:\n",
    "#         mapping = mapping | x.eq(label)\n",
    "#     return mapping\n",
    "\n",
    "# def compact1D(x):\n",
    "#     \"\"\"\n",
    "#     \"Compact\" values 1D uint tensor, so that all values are in [0, max(unique(x))].\n",
    "#     Args:\n",
    "#         x (Tensor): uint Tensor\n",
    "\n",
    "#     Returns:\n",
    "#         Tensor: uint Tensor of same shape as x\n",
    "\n",
    "#     Example:\n",
    "#         >>> densify1D(torch.ByteTensor([5, 8, 7, 3, 8, 42]))\n",
    "#         ByteTensor([1, 3, 2, 0, 3, 4])\n",
    "#     \"\"\"\n",
    "#     x_sorted, x_sorted_ind = torch.sort(x, descending=True)\n",
    "#     x_sorted_unique, x_sorted_unique_ind = torch.unique(x_sorted, return_inverse=True)\n",
    "#     x[x_sorted_ind] = x_sorted_unique_ind\n",
    "#     return x\n",
    "\n",
    "# # # Input sparse tensor:\n",
    "# # i = torch.from_numpy(np.array([[0,1,4,3,2,1],[0,1,3,1,4,1]]).astype(\"int64\"))\n",
    "# # v = torch.from_numpy(np.arange(1, 7).astype(\"float32\"))\n",
    "# # sparse_t = torch.sparse.FloatTensor(i, v)\n",
    "# # print(sparse_t.to_dense())\n",
    "# # tensor([[ 1.,  0.,  0.,  0.,  0.],\n",
    "# #         [ 0.,  8.,  0.,  0.,  0.],\n",
    "# #         [ 0.,  0.,  0.,  0.,  5.],\n",
    "# #         [ 0.,  4.,  0.,  0.,  0.],\n",
    "# #         [ 0.,  0.,  0.,  3.,  0.]])\n",
    "\n",
    "# # note: test1[1, 1] = v[i[1,:]] + v[i[6,:]] = 2 + 6 = 8\n",
    "# #       since both i[1,:] and i[6,:] are [1,1]\n",
    "\n",
    "# # Input slicing indices:\n",
    "# #     idx = [4,1,3]\n",
    "\n",
    "# tensor_idx = sparse_t._indices()\n",
    "# tensor_v = sparse_t._values()\n",
    "\n",
    "\n",
    "# # Getting the elements in `sparse_t` which correspond to `idx`:\n",
    "# existing_idx = in1D(tensor_idx, idx).byte()\n",
    "# print(existing_idx)\n",
    "# existing_idx = existing_idx.all(dim=1)\n",
    "# print(existing_idx)\n",
    "# existing_idx = existing_idx.nonzero().squeeze()\n",
    "# print(existing_idx)\n",
    "# v_sliced = tensor_v[existing_idx]\n",
    "# print(v_sliced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 8., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 5.],\n",
      "        [0., 4., 0., 0., 0.],\n",
      "        [0., 0., 0., 3., 0.]])\n",
      "v_idx tensor([[0, 1, 1, 1, 0, 1],\n",
      "        [0, 1, 1, 1, 1, 1]], dtype=torch.uint8)\n",
      "v_idx tensor([0, 1, 1, 1, 0, 1], dtype=torch.uint8)\n",
      "v_idx tensor([1, 2, 3, 5])\n",
      "v_sliced tensor([2., 3., 4., 6.])\n",
      "i_sliced tensor([[1, 4, 3, 1],\n",
      "        [1, 3, 1, 1]])\n",
      "tensor(indices=tensor([[0, 2, 1, 0],\n",
      "                       [0, 1, 0, 0]]),\n",
      "       values=tensor([2., 3., 4., 6.]),\n",
      "       size=(3, 3), nnz=4, layout=torch.sparse_coo)\n",
      "tensor([[8., 0., 0.],\n",
      "        [4., 0., 0.],\n",
      "        [0., 3., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def in1D(x, labels):\n",
    "    \"\"\"\n",
    "    Sub-optimal equivalent to numpy.in1D().\n",
    "    Hopefully this feature will be properly covered soon\n",
    "    c.f. https://github.com/pytorch/pytorch/issues/3025\n",
    "    Snippet by Aron Barreira Bordin\n",
    "    Args:\n",
    "        x (Tensor):             Tensor to search values in\n",
    "        labels (Tensor/list):   1D array of values to search for\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Boolean tensor y of same shape as x, with y[ind] = True if x[ind] in labels\n",
    "\n",
    "    Example:\n",
    "        >>> in1D(torch.FloatTensor([1, 2, 0, 3]), [2, 3])\n",
    "        FloatTensor([False, True, False, True])\n",
    "    \"\"\"\n",
    "    mapping = torch.zeros(x.size()).byte()\n",
    "    for label in labels:\n",
    "        mapping = mapping | x.eq(label)\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def compact1D(x):\n",
    "    \"\"\"\n",
    "    \"Compact\" values 1D uint tensor, so that all values are in [0, max(unique(x))].\n",
    "    Args:\n",
    "        x (Tensor): uint Tensor\n",
    "\n",
    "    Returns:\n",
    "        Tensor: uint Tensor of same shape as x\n",
    "\n",
    "    Example:\n",
    "        >>> densify1D(torch.ByteTensor([5, 8, 7, 3, 8, 42]))\n",
    "        ByteTensor([1, 3, 2, 0, 3, 4])\n",
    "    \"\"\"\n",
    "    x_sorted, x_sorted_ind = torch.sort(x, descending=True)\n",
    "    x_sorted_unique, x_sorted_unique_ind = torch.unique(x_sorted, return_inverse=True)\n",
    "    x[x_sorted_ind] = x_sorted_unique_ind\n",
    "    return x\n",
    "\n",
    "# Input sparse tensor:\n",
    "i = torch.from_numpy(np.array([[0,1,4,3,2,1],[0,1,3,1,4,1]]).astype(\"int64\"))\n",
    "v = torch.from_numpy(np.arange(1, 7).astype(\"float32\"))\n",
    "test1 = torch.sparse.FloatTensor(i, v)\n",
    "print(test1.to_dense())\n",
    "# tensor([[ 1.,  0.,  0.,  0.,  0.],\n",
    "#         [ 0.,  8.,  0.,  0.,  0.],\n",
    "#         [ 0.,  0.,  0.,  0.,  5.],\n",
    "#         [ 0.,  4.,  0.,  0.,  0.],\n",
    "#         [ 0.,  0.,  0.,  3.,  0.]])\n",
    "\n",
    "# note: test1[1, 1] = v[i[1,:]] + v[i[6,:]] = 2 + 6 = 8\n",
    "#       since both i[1,:] and i[6,:] are [1,1]\n",
    "\n",
    "# Input slicing indices:\n",
    "idx = [4,1,3]\n",
    "\n",
    "# Getting the elements in `i` which correspond to `idx`:\n",
    "v_idx = in1D(i, idx).byte()\n",
    "print('v_idx', v_idx)\n",
    "v_idx = v_idx.sum(dim=0).squeeze() == i.size(0) # or `v_idx.all(dim=1)` for pytorch 0.5+\n",
    "\n",
    "print('v_idx', v_idx)\n",
    "v_idx = v_idx.nonzero().squeeze()\n",
    "print('v_idx', v_idx)\n",
    "\n",
    "# Slicing `v` and `i` accordingly:\n",
    "v_sliced = v[v_idx]\n",
    "print('v_sliced', v_sliced)\n",
    "i_sliced = i.index_select(dim=1, index=v_idx)\n",
    "print('i_sliced', i_sliced)\n",
    "\n",
    "# Building sparse result tensor:\n",
    "i_sliced[0] = compact1D(i_sliced[0])\n",
    "i_sliced[1] = compact1D(i_sliced[1])\n",
    "\n",
    "# To make sure to have a square dense representation:\n",
    "size_sliced = torch.Size([len(idx), len(idx)])\n",
    "res = torch.sparse.FloatTensor(i_sliced, v_sliced, size_sliced)\n",
    "\n",
    "print(res)\n",
    "# torch.sparse.FloatTensor of size (3,3) with indices:\n",
    "# tensor([[ 0,  2,  1,  0],\n",
    "#         [ 0,  1,  0,  0]])\n",
    "# and values:\n",
    "# tensor([ 2.,  3.,  4.,  6.])\n",
    "\n",
    "print(res.to_dense())\n",
    "# tensor([[ 8.,  0.,  0.],\n",
    "#         [ 4.,  0.,  0.],\n",
    "#         [ 0.,  3.,  0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make basis_projection layers \n",
    "* Assume the model has self.basis_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dProjection(nn.Conv2d):\n",
    "    \"\"\" 2D convolutions but only optimizing subspace\n",
    "    \"\"\"\n",
    "    def __init__(self, d, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(Conv2dProjection, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, 0, dilation,\n",
    "            groups, bias)\n",
    "        \n",
    "        self.d = d\n",
    "        self.weight.requires_grad = False\n",
    "        if self.bias is not None:\n",
    "            self.bias.requires_grad = False\n",
    "        self.D = 0\n",
    "        self.D += torch.prod(torch.tensor(self.weight.size()))\n",
    "        if self.bias is not None:\n",
    "            self.D += torch.prod(torch.tensor(layer.bias.size()))\n",
    "            \n",
    "    def get_projection_matrix(self):\n",
    "        self.proj_matrix = nn.Parameter(torch.randn(self.D, self.d).to(DEVICE), requires_grad=False)\n",
    "        self.proj_matrix = F.normalize(proj_matrix, dim=0, p=2)\n",
    "        \n",
    "        \n",
    "        self.proj_matrix_w = nn.Parameter(torch.randn(torch.prod(torch.tensor(self.weight.size())), \n",
    "                                                    self.d).to(DEVICE), requires_grad=False)\n",
    "        self.proj_matrix_w = F.normalize(self.proj_matrix_w, dim=1, p=2)\n",
    "        self.proj_matrix_w.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # Turn it into a nice format for a weight and bias matrix for each layer\n",
    "        param_idx = 0\n",
    "        self.projection = {}\n",
    "        for name, layer in self.layers.items():\n",
    "            self.projection[name] = {}\n",
    "            n_weight_params = torch.prod(torch.tensor(layer.weight.size()))\n",
    "            self.projection[name]['weight'] = proj_matrix[param_idx : param_idx + n_weight_params, :]\n",
    "            self.projection[name]['weight'].requires_grad = False\n",
    "            # also make sure the layers aren't trainable\n",
    "            self.layers[name].weight.requires_grad = False\n",
    "            param_idx += n_weight_params\n",
    "            if layer.bias is not None:\n",
    "                n_bias_params = torch.prod(torch.tensor(layer.bias.size()))\n",
    "                self.projection[name]['bias'] = proj_matrix[param_idx : param_idx + n_bias_params, :]\n",
    "                self.projection[name]['bias'].requires_grad = False\n",
    "                # also make sure the layers aren't trainable\n",
    "                self.layers[name].bias.requires_grad = False\n",
    "                param_idx += n_bias_params\n",
    "#         print(f'Model contains {self.D} params, but only optimizing a {self.d} dim subspace')\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        ih, iw = x.size()[-2:]\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        oh = math.ceil(ih / self.stride[0])\n",
    "        ow = math.ceil(iw / self.stride[1])\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, [pad_w//2, pad_w - pad_w//2, pad_h//2, pad_h - pad_h//2])\n",
    "        return F.conv2d(x, self.weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class IntrinsicNet(nn.Module):\n",
    "    ''' Optimize subspace of parameters of network defined as  using dense projection.\n",
    "    \n",
    "    d: intrinsic dimension size to test\n",
    "\n",
    "    ??? This still won't work for resnets, or any other more complex network\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d, config, DEVICE='cuda'):\n",
    "        super(IntrinsicNet, self).__init__()\n",
    "        self.d = d\n",
    "        self.opt_basis = nn.Parameter(torch.zeros(self.d).to(DEVICE), requires_grad=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4019, -0.7336],\n",
      "        [-0.8620, -1.0364],\n",
      "        [ 0.5238, -1.1870]])\n",
      "--------\n",
      "tensor([[ 0.8117, -0.4221],\n",
      "        [-0.4991, -0.5962],\n",
      "        [ 0.3033, -0.6829]])\n",
      "--------\n",
      "tensor([1., 1.])\n",
      "tensor([0.9149, 0.7776, 0.7472])\n",
      "--------\n",
      "tensor([1.2319, 2.3693])\n",
      "tensor([1.4142, 1.5407, 1.6606])\n"
     ]
    }
   ],
   "source": [
    "proj_matrix = torch.randn(3, 2)\n",
    "print(proj_matrix)\n",
    "print('--------')\n",
    "proj_matrix_n = F.normalize(proj_matrix, dim=0, p=2)\n",
    "print(proj_matrix_n)\n",
    "print('--------')\n",
    "print(torch.norm(proj_matrix_n, dim=0))\n",
    "print(torch.norm(proj_matrix_n, dim=1))\n",
    "print('--------')\n",
    "\n",
    "def normalize(x):\n",
    "    x_normed = x / x.max(0, keepdim=True)[0]\n",
    "    return x_normed\n",
    "\n",
    "print(torch.norm(normalize(proj_matrix), dim=0))\n",
    "print(torch.norm(normalize(proj_matrix), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HVAE",
   "language": "python",
   "name": "hvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
